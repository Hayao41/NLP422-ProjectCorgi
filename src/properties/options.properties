{
    # embedding layer params
    "word_emb_dims": 0,
    "pos_emb_dims": 25,
    "rel_emb_dims": 25,
    "rp_emb_dims": 0,

    # non linear trans
    "context_linear_dim": 50,

    # context encoder
    "use_bi_lstm": True,
    "lstm_num_layers": 1,
    "lstm_hid_dims": 100,

    # tree children chain
    "use_bi_chain": False,
    "chain_num_layers": 1,
    "chain_hid_dims": 100,

    # optimization
    "batch_size": 4,
    "epoch": 30,
    "xavier": True,
    "dropout": 0.25,
    "padding": 0,
    "use_cuda": False,
    "cuda_device": "1",
    "optim": "Adam",
    "lr": 0.0001,
    "lr_decay": 0.1,
    "weight_decay": 0.0001,
    "momentum": 0.5,
    "betas": (0.9, 0.98),
    "eps": 1e-9
}