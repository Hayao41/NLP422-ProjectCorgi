{
    # embedding layer params
    "word_emb_dims": 0,
    "pos_emb_dims": 25,
    "rel_emb_dims": 25,
    "rp_emb_dims": 0,

    # non linear trans
    "context_linear_dim": 50,

    # context encoder
    "use_bi_lstm": True,
    "lstm_num_layers": 2,
    "lstm_hid_dims": 100,

    # tree children chain
    "use_bi_chain": False,
    "chain_num_layers": 2,
    "chain_hid_dims": 100,

    # optimization
    "batch_size": 5,
    "xavier": True,
    "dropout": 0.1,
    "padding": 0,
    "use_cuda": False
}